{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzH7tgStJksW"
      },
      "source": [
        "Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M5HyqW2FJioP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "Using device: cuda\n",
            "CUDA device name: NVIDIA GeForce RTX 3070 Ti Laptop GPU\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "from copy import deepcopy\n",
        "import matplotlib.pyplot as plt\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(torch.cuda.is_available())\n",
        "print(\"Using device:\", device)\n",
        "if device.type == \"cuda\":\n",
        "    print(\"CUDA device name:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def to_device(tensor):\n",
        "    return tensor.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1c6KRsCJpvG"
      },
      "source": [
        "Class for connect4 environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfxUylEHJw5Q"
      },
      "outputs": [],
      "source": [
        "class Connect4Env:\n",
        "    def __init__(self):\n",
        "        self.rows = 6\n",
        "        self.cols = 7\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = np.zeros((self.rows, self.cols), dtype=int)\n",
        "        self.current_player = 1\n",
        "        self.last_move = None\n",
        "        return self._get_obs()\n",
        "\n",
        "    def _get_obs(self):\n",
        "        p1_board = (self.board == 1).astype(np.float32)\n",
        "        p2_board = (self.board == -1).astype(np.float32)\n",
        "        if self.current_player == 1:\n",
        "            return np.stack([p1_board, p2_board], axis=0)\n",
        "        else:\n",
        "            return np.stack([p2_board, p1_board], axis=0)\n",
        "\n",
        "    def valid_actions(self):\n",
        "        return [c for c in range(self.cols) if self.board[0, c] == 0]\n",
        "\n",
        "    def step(self, action):\n",
        "        if action not in self.valid_actions():\n",
        "            return self._get_obs(), -10, True, {}\n",
        "\n",
        "        for r in range(self.rows - 1, -1, -1):\n",
        "            if self.board[r, action] == 0:\n",
        "                self.board[r, action] = self.current_player\n",
        "                self.last_move = (r, action)\n",
        "                break\n",
        "\n",
        "        if self._check_winner(self.current_player, self.last_move):\n",
        "            return self._get_obs(), 1, True, {}\n",
        "\n",
        "        if len(self.valid_actions()) == 0:\n",
        "            return self._get_obs(), 0, True, {}\n",
        "\n",
        "        self.current_player *= -1\n",
        "        return self._get_obs(), -0.01, False, {}\n",
        "\n",
        "    def _check_winner(self, player, last_move):\n",
        "        if last_move is None:\n",
        "            return False\n",
        "        r, c = last_move\n",
        "        directions = [(1, 0), (0, 1), (1, 1), (1, -1)]\n",
        "        for dr, dc in directions:\n",
        "            count = 1\n",
        "            for d in [-1, 1]:\n",
        "                nr, nc = r + d * dr, c + d * dc\n",
        "                while 0 <= nr < self.rows and 0 <= nc < self.cols and self.board[nr, nc] == player:\n",
        "                    count += 1\n",
        "                    if count >= 4:\n",
        "                        return True\n",
        "                    nr += d * dr\n",
        "                    nc += d * dc\n",
        "        return False\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJdsV1swJ3lg"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YaLFrL97J3QU"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(2, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 6 * 7, 128)\n",
        "        self.out = nn.Linear(128, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.out(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xljipnuZKGS1"
      },
      "source": [
        "Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Lf85V8OwKJta"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csLuvB18KKjR"
      },
      "source": [
        "Selecting Action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AQMGWMxsKP1v"
      },
      "outputs": [],
      "source": [
        "# --- Training Functions ---\n",
        "def select_action(model, state, epsilon, valid_actions):\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice(valid_actions)\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            q_values = model(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device))\n",
        "            q_values = q_values.squeeze()\n",
        "            q_values[[i for i in range(7) if i not in valid_actions]] = -float('inf')\n",
        "            return torch.argmax(q_values).item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_i6kGwpKSZ_"
      },
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "De4_ZhldKU00"
      },
      "outputs": [],
      "source": [
        "def train_self_play(\n",
        "    episodes=50000, epsilon=1.0, epsilon_min=0.1, epsilon_decay=0.995,\n",
        "    gamma=0.99, batch_size=64, update_target_every=10\n",
        "):\n",
        "    env = Connect4Env()\n",
        "    model1 = DQN().to(device)\n",
        "    model2 = DQN().to(device)\n",
        "    target_model1 = DQN().to(device)\n",
        "    target_model2 = DQN().to(device)\n",
        "    target_model1.load_state_dict(model1.state_dict())\n",
        "    target_model2.load_state_dict(model2.state_dict())\n",
        "    optimizer1 = optim.Adam(model1.parameters(), lr=1e-3)\n",
        "    optimizer2 = optim.Adam(model2.parameters(), lr=1e-3)\n",
        "    buffer1 = ReplayBuffer()\n",
        "    buffer2 = ReplayBuffer()\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        reward_p1 = 0\n",
        "        reward_p2 = 0\n",
        "        last_moves = {1: None, -1: None}  # track last move per player\n",
        "\n",
        "        while not done:\n",
        "            current_player = env.current_player\n",
        "            model = model1 if current_player == 1 else model2\n",
        "            buffer = buffer1 if current_player == 1 else buffer2\n",
        "            optimizer = optimizer1 if current_player == 1 else optimizer2\n",
        "            target_model = target_model1 if current_player == 1 else target_model2\n",
        "\n",
        "            valid_actions = env.valid_actions()\n",
        "            action = select_action(model, state, epsilon, valid_actions)\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            # Store last move for possible later penalty\n",
        "            last_moves[current_player] = (np.array(state, copy=True), action, np.array(next_state, copy=True))\n",
        "\n",
        "            # Store current player's experience\n",
        "            buffer.push((np.array(state, copy=True), action, reward, np.array(next_state, copy=True), done))\n",
        "            state = next_state\n",
        "\n",
        "            # Track cumulative reward\n",
        "            if current_player == 1:\n",
        "                reward_p1 += reward\n",
        "                if done and reward == 1 and last_moves[-1] is not None:\n",
        "                    reward_p2 += -1.0  # opponent lost\n",
        "            else:\n",
        "                reward_p2 += reward\n",
        "                if done and reward == 1 and last_moves[1] is not None:\n",
        "                    reward_p1 += -1.0  # opponent lost\n",
        "\n",
        "            # If game is over and current player won, penalize opponent\n",
        "            if done and reward == 1:\n",
        "                losing_player = -current_player\n",
        "                if last_moves[losing_player] is not None:\n",
        "                    s, a, ns = last_moves[losing_player]\n",
        "                    opp_buffer = buffer1 if losing_player == 1 else buffer2\n",
        "                    opp_buffer.push((s, a, -1.0, ns, True))\n",
        "\n",
        "            # Train on batch if buffer is ready\n",
        "            if len(buffer) >= batch_size:\n",
        "                transitions = buffer.sample(batch_size)\n",
        "                states, actions, rewards, next_states, dones = zip(*transitions)\n",
        "\n",
        "                states = torch.tensor(np.stack(states), dtype=torch.float32).to(device)\n",
        "                actions = torch.tensor(actions).unsqueeze(1).to(device)\n",
        "                rewards = torch.tensor(rewards).unsqueeze(1).to(device)\n",
        "                next_states = torch.tensor(np.stack(next_states), dtype=torch.float32).to(device)\n",
        "                dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "\n",
        "                q_values = model(states).gather(1, actions)\n",
        "                with torch.no_grad():\n",
        "                    max_next_q = target_model(next_states).max(1)[0].unsqueeze(1)\n",
        "                    target_q = rewards + gamma * max_next_q * (1 - dones)\n",
        "\n",
        "                loss = F.mse_loss(q_values, target_q)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        # Epsilon decay\n",
        "        if epsilon > epsilon_min:\n",
        "            epsilon *= epsilon_decay\n",
        "\n",
        "        # Update target models periodically\n",
        "        if episode % update_target_every == 0:\n",
        "            target_model1.load_state_dict(model1.state_dict())\n",
        "            target_model2.load_state_dict(model2.state_dict())\n",
        "\n",
        "        # Logging\n",
        "        if episode % 100 == 0:\n",
        "            print(f\"Episode {episode}, P1 reward: {reward_p1:.2f}, P2 reward: {reward_p2:.2f}, Epsilon: {epsilon:.3f}\")\n",
        "\n",
        "        # Periodic saving\n",
        "        if episode % 10000 == 0 and episode > 0:\n",
        "            torch.save(model1.state_dict(), f\"connect4_model1_ep{episode}.pth\")\n",
        "            torch.save(model2.state_dict(), f\"connect4_model2_ep{episode}.pth\")\n",
        "\n",
        "    # Save final models\n",
        "    torch.save(model1.state_dict(), \"connect4_model1_final.pth\")\n",
        "    torch.save(model2.state_dict(), \"connect4_model2_final.pth\")\n",
        "\n",
        "    return model1, model2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXKXWYDSKYn1"
      },
      "source": [
        "Train It"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "8aeN-tkFKaRm",
        "outputId": "81958218-079e-4436-a746-783ec15a9370"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Penalized Player -1 for losing\n",
            "Episode 0, P1 reward: 0.84, P2 reward: -1.16, Epsilon: 1.000\n",
            "Penalized Player 1 for losing\n",
            "Penalized Player -1 for losing\n",
            "Penalized Player -1 for losing\n",
            "Penalized Player -1 for losing\n",
            "Penalized Player -1 for losing\n",
            "Penalized Player 1 for losing\n",
            "Penalized Player 1 for losing\n",
            "Penalized Player 1 for losing\n",
            "Penalized Player 1 for losing\n",
            "Penalized Player -1 for losing\n",
            "Penalized Player -1 for losing\n",
            "Penalized Player -1 for losing\n",
            "Penalized Player -1 for losing\n",
            "Penalized Player 1 for losing\n",
            "Penalized Player 1 for losing\n",
            "Penalized Player 1 for losing\n",
            "Penalized Player -1 for losing\n",
            "Penalized Player 1 for losing\n",
            "Penalized Player 1 for losing\n",
            "Penalized Player -1 for losing\n",
            "Penalized Player 1 for losing\n",
            "Penalized Player -1 for losing\n",
            "Penalized Player -1 for losing\n",
            "Penalized Player 1 for losing\n",
            "Penalized Player 1 for losing\n",
            "Penalized Player -1 for losing\n",
            "Penalized Player -1 for losing\n",
            "Penalized Player 1 for losing\n",
            "Penalized Player -1 for losing\n",
            "Penalized Player -1 for losing\n",
            "Penalized Player -1 for losing\n",
            "Penalized Player 1 for losing\n",
            "Penalized Player 1 for losing\n",
            "Penalized Player -1 for losing\n",
            "Penalized Player -1 for losing\n",
            "Penalized Player -1 for losing\n",
            "Penalized Player 1 for losing\n",
            "Penalized Player -1 for losing\n",
            "Penalized Player 1 for losing\n",
            "Penalized Player 1 for losing\n",
            "Penalized Player -1 for losing\n",
            "Penalized Player -1 for losing\n",
            "Penalized Player -1 for losing\n",
            "Penalized Player 1 for losing\n",
            "Penalized Player 1 for losing\n",
            "Penalized Player -1 for losing\n",
            "Penalized Player -1 for losing\n",
            "Penalized Player 1 for losing\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m trained_model1, trained_model2 = \u001b[43mtrain_self_play\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mepsilon_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9995\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mtrain_self_play\u001b[39m\u001b[34m(episodes, epsilon, epsilon_min, epsilon_decay, gamma, batch_size, update_target_every)\u001b[39m\n\u001b[32m     65\u001b[39m states, actions, rewards, next_states, dones = \u001b[38;5;28mzip\u001b[39m(*transitions)\n\u001b[32m     67\u001b[39m states = torch.tensor(np.stack(states), dtype=torch.float32).to(device)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m actions = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m rewards = torch.tensor(rewards).unsqueeze(\u001b[32m1\u001b[39m).to(device)\n\u001b[32m     70\u001b[39m next_states = torch.tensor(np.stack(next_states), dtype=torch.float32).to(device)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "trained_model1, trained_model2 = train_self_play(episodes=100000,epsilon_decay=0.9995)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_model(model, path):\n",
        "    torch.save(model.state_dict(), path)\n",
        "    print(f\"Model saved to {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'trained_model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m save_model(\u001b[43mtrained_model\u001b[49m)\n",
            "\u001b[31mNameError\u001b[39m: name 'trained_model' is not defined"
          ]
        }
      ],
      "source": [
        "save_model(trained_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_trained_model(path=\"connect4_model_weights.pth\"):\n",
        "    model = DQN().to(device)\n",
        "    model.load_state_dict(torch.load(path, map_location=device))\n",
        "    model.eval()  # Set to evaluation mode\n",
        "    print(f\"Loaded model from {path}\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded model from connect4_dqn_ep10000.pth\n"
          ]
        }
      ],
      "source": [
        "trained_model = load_trained_model(path=\"connect4_dqn_ep10000.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32aOgNMqOSDp"
      },
      "source": [
        "Playing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NGqA-mdHOTFx"
      },
      "outputs": [],
      "source": [
        "def play_against_model(starting_player=None):\n",
        "    model_choice = input(\"Play against model 1 (X) or model 2 (O)? Enter 1 or 2: \").strip()\n",
        "    model_path = \"connect4_model1_ep20000.pth\" if model_choice == \"1\" else \"connect4_model2_ep20000.pth\"\n",
        "    model = DQN().to(device)\n",
        "    env = Connect4Env()\n",
        "\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    if starting_player is None:\n",
        "        choice = input(\"Do you want to go first? (y/n): \").lower()\n",
        "        if choice == 'y':\n",
        "            env.current_player = -1\n",
        "        else:\n",
        "            env.current_player = 1\n",
        "    else:\n",
        "        env.current_player = -1 if starting_player == -1 else 1\n",
        "    print(\"You are Player -1 (O). Model is Player 1 (X). Columns: 0 to 6\")\n",
        "    print(f\"Playing against model from: {model_path}\")\n",
        "    print(env.board)\n",
        "\n",
        "    while not done:\n",
        "        if env.current_player == -1:\n",
        "            try:\n",
        "                user_action = int(input(\"Your move (0-6): \"))\n",
        "            except ValueError:\n",
        "                print(\"Invalid input. Try a number from 0 to 6.\")\n",
        "                continue\n",
        "            if user_action not in env.valid_actions():\n",
        "                print(\"Invalid move. Try again.\")\n",
        "                continue\n",
        "            state, reward, done, _ = env.step(user_action)\n",
        "        else:\n",
        "            valid_actions = env.valid_actions()\n",
        "            action = select_action(model, state, epsilon=0.0, valid_actions=valid_actions)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            print(\"Model played column:\", action)\n",
        "\n",
        "        # Always print board after any move\n",
        "        symbol_map = {1: 'X', -1: 'O', 0: '.'}\n",
        "        print(\"\\nCurrent board:\")\n",
        "        for row in env.board:\n",
        "            print(' '.join(symbol_map[cell] for cell in row))\n",
        "        print()\n",
        "\n",
        "        if done:\n",
        "            print(\"Game Over. Reward:\", reward)\n",
        "            break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scFE5r_6ObKi",
        "outputId": "f2b45771-82e3-4fd7-aec4-8dccf6e35df5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ajmat\\AppData\\Local\\Temp\\ipykernel_77264\\3961894182.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are Player -1 (O). Model is Player 1 (X). Columns: 0 to 6\n",
            "Playing against model from: connect4_model2_ep20000.pth\n",
            "[[0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n",
            "Model played column: 4\n",
            "\n",
            "Current board:\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . X . .\n",
            "\n",
            "\n",
            "Current board:\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . O X . .\n",
            "\n",
            "Model played column: 1\n",
            "\n",
            "Current board:\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". X . O X . .\n",
            "\n",
            "\n",
            "Current board:\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . O . . .\n",
            ". X . O X . .\n",
            "\n",
            "Model played column: 4\n",
            "\n",
            "Current board:\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . O X . .\n",
            ". X . O X . .\n",
            "\n",
            "\n",
            "Current board:\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . O . . .\n",
            ". . . O X . .\n",
            ". X . O X . .\n",
            "\n",
            "Model played column: 1\n",
            "\n",
            "Current board:\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . O . . .\n",
            ". X . O X . .\n",
            ". X . O X . .\n",
            "\n",
            "\n",
            "Current board:\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . O . . .\n",
            ". . . O . . .\n",
            ". X . O X . .\n",
            ". X . O X . .\n",
            "\n",
            "Game Over. Reward: 1\n"
          ]
        }
      ],
      "source": [
        "play_against_model()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
