{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzH7tgStJksW"
      },
      "source": [
        "Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M5HyqW2FJioP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "Using device: cuda\n",
            "CUDA device name: NVIDIA GeForce RTX 3070 Ti Laptop GPU\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "from copy import deepcopy\n",
        "import matplotlib.pyplot as plt\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(torch.cuda.is_available())\n",
        "print(\"Using device:\", device)\n",
        "if device.type == \"cuda\":\n",
        "    print(\"CUDA device name:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def to_device(tensor):\n",
        "    return tensor.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1c6KRsCJpvG"
      },
      "source": [
        "Class for connect4 environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QfxUylEHJw5Q"
      },
      "outputs": [],
      "source": [
        "class Connect4Env:\n",
        "    def __init__(self):\n",
        "        self.rows = 6\n",
        "        self.cols = 7\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = np.zeros((self.rows, self.cols), dtype=int)\n",
        "        self.current_player = 1\n",
        "        self.last_move = None\n",
        "        return self._get_obs()\n",
        "\n",
        "    def _get_obs(self):\n",
        "        p1_board = (self.board == 1).astype(np.float32)\n",
        "        p2_board = (self.board == -1).astype(np.float32)\n",
        "        return np.stack([p1_board, p2_board], axis=0) if self.current_player == 1 else np.stack([p2_board, p1_board], axis=0)\n",
        "\n",
        "    def valid_actions(self):\n",
        "        return [c for c in range(self.cols) if self.board[0, c] == 0]\n",
        "\n",
        "    def step(self, action):\n",
        "        if action not in self.valid_actions():\n",
        "            return self._get_obs(), -10, True, {}\n",
        "\n",
        "        for r in range(self.rows - 1, -1, -1):\n",
        "            if self.board[r, action] == 0:\n",
        "                self.board[r, action] = self.current_player\n",
        "                self.last_move = (r, action)\n",
        "                break\n",
        "\n",
        "        if self._check_winner(self.current_player, self.last_move):\n",
        "            return self._get_obs(), 1, True, {}\n",
        "\n",
        "        if len(self.valid_actions()) == 0:\n",
        "            return self._get_obs(), 0, True, {}\n",
        "\n",
        "        self.current_player *= -1\n",
        "        return self._get_obs(), -0.005, False, {}\n",
        "\n",
        "    def _check_winner(self, player, last_move):\n",
        "        if last_move is None:\n",
        "            return False\n",
        "        r, c = last_move\n",
        "        directions = [(1, 0), (0, 1), (1, 1), (1, -1)]\n",
        "        for dr, dc in directions:\n",
        "            count = 1\n",
        "            for d in [-1, 1]:\n",
        "                nr, nc = r + d * dr, c + d * dc\n",
        "                while 0 <= nr < self.rows and 0 <= nc < self.cols and self.board[nr, nc] == player:\n",
        "                    count += 1\n",
        "                    if count >= 4:\n",
        "                        return True\n",
        "                    nr += d * dr\n",
        "                    nc += d * dc\n",
        "        return False\n",
        "\n",
        "    @staticmethod\n",
        "    def _check_winner_static(board, player):\n",
        "        for r in range(6):\n",
        "            for c in range(4):\n",
        "                if all(board[r, c+i] == player for i in range(4)):\n",
        "                    return True\n",
        "        for c in range(7):\n",
        "            for r in range(3):\n",
        "                if all(board[r+i, c] == player for i in range(4)):\n",
        "                    return True\n",
        "        for r in range(3):\n",
        "            for c in range(4):\n",
        "                if all(board[r+i, c+i] == player for i in range(4)):\n",
        "                    return True\n",
        "        for r in range(3):\n",
        "            for c in range(3, 7):\n",
        "                if all(board[r+i, c-i] == player for i in range(4)):\n",
        "                    return True\n",
        "        return False\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_threats_from_move(board, player, move):\n",
        "    r, c = move\n",
        "    rows, cols = 6, 7\n",
        "    threats = set()\n",
        "    directions = [(0,1), (1,0), (1,1), (1,-1)]\n",
        "\n",
        "    for dr, dc in directions:\n",
        "        for offset in range(-3, 1):\n",
        "            segment = []\n",
        "            positions = []\n",
        "            for i in range(4):\n",
        "                nr = r + (offset + i) * dr\n",
        "                nc = c + (offset + i) * dc\n",
        "                if 0 <= nr < rows and 0 <= nc < cols:\n",
        "                    segment.append(board[nr, nc])\n",
        "                    positions.append((nr, nc))\n",
        "            if len(segment) == 4 and segment.count(player) == 3 and segment.count(0) == 1:\n",
        "                idx = segment.index(0)\n",
        "                er, ec = positions[idx]\n",
        "                if er == rows - 1 or board[er + 1, ec] != 0:\n",
        "                    threats.add(ec)\n",
        "    return threats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_move(prev_board, new_board, current_player, valid_actions, last_move):\n",
        "    reward = 0.0\n",
        "\n",
        "    threats = find_threats_from_move(new_board, current_player, last_move)\n",
        "    if len(threats) >= 2:\n",
        "        reward += 0.6\n",
        "    elif len(threats) == 1:\n",
        "        reward += 0.3\n",
        "\n",
        "    for action in valid_actions:\n",
        "        temp = new_board.copy()\n",
        "        for r in range(5, -1, -1):\n",
        "            if temp[r, action] == 0:\n",
        "                temp[r, action] = current_player\n",
        "                break\n",
        "        if Connect4Env._check_winner_static(temp, current_player):\n",
        "            reward -= 0.25\n",
        "            break\n",
        "\n",
        "    opponent = -current_player\n",
        "    blocked = False\n",
        "    for action in valid_actions:\n",
        "        temp = prev_board.copy()\n",
        "        for r in range(5, -1, -1):\n",
        "            if temp[r, action] == 0:\n",
        "                temp[r, action] = opponent\n",
        "                break\n",
        "        if Connect4Env._check_winner_static(temp, opponent):\n",
        "            temp2 = new_board.copy()\n",
        "            for r in range(5, -1, -1):\n",
        "                if temp2[r, action] == 0:\n",
        "                    temp2[r, action] = opponent\n",
        "                    break\n",
        "            if not Connect4Env._check_winner_static(temp2, opponent):\n",
        "                reward += 0.3\n",
        "                blocked = True\n",
        "                break\n",
        "    if not blocked:\n",
        "        reward -= 0.2\n",
        "\n",
        "    return reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_true_threes(board, player):\n",
        "    count = 0\n",
        "    # Horizontal\n",
        "    for row in range(6):\n",
        "        for col in range(4):\n",
        "            line = board[row, col:col+4]\n",
        "            if np.count_nonzero(line == player) == 3 and np.count_nonzero(line == 0) == 1:\n",
        "                if is_contiguous(line, player):\n",
        "                    count += 1\n",
        "    # Vertical\n",
        "    for col in range(7):\n",
        "        for row in range(3):\n",
        "            line = board[row:row+4, col]\n",
        "            if np.count_nonzero(line == player) == 3 and np.count_nonzero(line == 0) == 1:\n",
        "                if is_contiguous(line, player):\n",
        "                    count += 1\n",
        "    # Diagonal (\\)\n",
        "    for row in range(3):\n",
        "        for col in range(4):\n",
        "            line = [board[row+i, col+i] for i in range(4)]\n",
        "            if line.count(player) == 3 and line.count(0) == 1:\n",
        "                if is_contiguous(line, player):\n",
        "                    count += 1\n",
        "    # Diagonal (/)\n",
        "    for row in range(3):\n",
        "        for col in range(3, 7):\n",
        "            line = [board[row+i, col-i] for i in range(4)]\n",
        "            if line.count(player) == 3 and line.count(0) == 1:\n",
        "                if is_contiguous(line, player):\n",
        "                    count += 1\n",
        "    return count\n",
        "\n",
        "def is_contiguous(line, player):\n",
        "    # True if there are no gaps between player pieces in the 4-element line\n",
        "    indexes = [i for i, val in enumerate(line) if val == player]\n",
        "    return max(indexes) - min(indexes) <= 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJdsV1swJ3lg"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaLFrL97J3QU"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(2, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 6 * 7, 128)\n",
        "        self.out = nn.Linear(128, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.out(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xljipnuZKGS1"
      },
      "source": [
        "Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lf85V8OwKJta"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csLuvB18KKjR"
      },
      "source": [
        "Selecting Action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQMGWMxsKP1v"
      },
      "outputs": [],
      "source": [
        "# --- Training Functions ---\n",
        "def select_action(model, state, epsilon, valid_actions):\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice(valid_actions)\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            q_values = model(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)).squeeze()\n",
        "            q_values[[i for i in range(7) if i not in valid_actions]] = -float('inf')\n",
        "            return torch.argmax(q_values).item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_i6kGwpKSZ_"
      },
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "De4_ZhldKU00"
      },
      "outputs": [],
      "source": [
        "def train_self_play(episodes=50000, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.9995,\n",
        "                    gamma=0.99, batch_size=64, update_target_every=10):\n",
        "    env = Connect4Env()\n",
        "    model1 = DQN().to(device)\n",
        "    model2 = DQN().to(device)\n",
        "    target_model1 = DQN().to(device)\n",
        "    target_model2 = DQN().to(device)\n",
        "    target_model1.load_state_dict(model1.state_dict())\n",
        "    target_model2.load_state_dict(model2.state_dict())\n",
        "    optimizer1 = optim.Adam(model1.parameters(), lr=1e-3)\n",
        "    optimizer2 = optim.Adam(model2.parameters(), lr=1e-3)\n",
        "    buffer1 = ReplayBuffer()\n",
        "    buffer2 = ReplayBuffer()\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        reward_p1 = 0\n",
        "        reward_p2 = 0\n",
        "        last_moves = {1: None, -1: None}\n",
        "\n",
        "        while not done:\n",
        "            current_player = env.current_player\n",
        "            model = model1 if current_player == 1 else model2\n",
        "            buffer = buffer1 if current_player == 1 else buffer2\n",
        "            optimizer = optimizer1 if current_player == 1 else optimizer2\n",
        "            target_model = target_model1 if current_player == 1 else target_model2\n",
        "\n",
        "            valid_actions = env.valid_actions()\n",
        "            action = select_action(model, state, epsilon, valid_actions)\n",
        "\n",
        "            prev_board = env.board.copy()\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            reward += analyze_move(prev_board, env.board, current_player, env.valid_actions(), env.last_move)\n",
        "\n",
        "            last_moves[current_player] = (np.array(state, copy=True), action, np.array(next_state, copy=True))\n",
        "            buffer.push((np.array(state, copy=True), action, reward, np.array(next_state, copy=True), done))\n",
        "            state = next_state\n",
        "\n",
        "            if current_player == 1:\n",
        "                reward_p1 += reward\n",
        "            else:\n",
        "                reward_p2 += reward\n",
        "\n",
        "            if done and reward >= 1:\n",
        "                losing_player = -current_player\n",
        "                if last_moves[losing_player] is not None:\n",
        "                    s, a, ns = last_moves[losing_player]\n",
        "                    opp_buffer = buffer1 if losing_player == 1 else buffer2\n",
        "                    opp_buffer.push((s, a, -1.0, ns, True))\n",
        "                    if losing_player == 1:\n",
        "                        reward_p1 += -1.0\n",
        "                    else:\n",
        "                        reward_p2 += -1.0\n",
        "\n",
        "            if len(buffer) >= batch_size:\n",
        "                transitions = buffer.sample(batch_size)\n",
        "                states, actions, rewards, next_states, dones = zip(*transitions)\n",
        "                states = torch.tensor(np.stack(states), dtype=torch.float32).to(device)\n",
        "                actions = torch.tensor(actions).unsqueeze(1).to(device)\n",
        "                rewards = torch.tensor(rewards).unsqueeze(1).to(device)\n",
        "                next_states = torch.tensor(np.stack(next_states), dtype=torch.float32).to(device)\n",
        "                dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "\n",
        "                q_values = model(states).gather(1, actions)\n",
        "                with torch.no_grad():\n",
        "                    max_next_q = target_model(next_states).max(1)[0].unsqueeze(1)\n",
        "                    target_q = rewards + gamma * max_next_q * (1 - dones)\n",
        "\n",
        "                loss = F.mse_loss(q_values, target_q)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        if epsilon > epsilon_min:\n",
        "            epsilon *= epsilon_decay\n",
        "\n",
        "        if episode % update_target_every == 0:\n",
        "            target_model1.load_state_dict(model1.state_dict())\n",
        "            target_model2.load_state_dict(model2.state_dict())\n",
        "\n",
        "        if episode % 100 == 0:\n",
        "            print(f\"Episode {episode}, P1 reward: {reward_p1:.2f}, P2 reward: {reward_p2:.2f}, Epsilon: {epsilon:.3f}\")\n",
        "\n",
        "    torch.save(model1.state_dict(), \"connect4_model1_final.pth\")\n",
        "    torch.save(model2.state_dict(), \"connect4_model2_final.pth\")\n",
        "    return model1, model2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXKXWYDSKYn1"
      },
      "source": [
        "Train It"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "8aeN-tkFKaRm",
        "outputId": "81958218-079e-4436-a746-783ec15a9370"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 0, P1 reward: 3.30, P2 reward: -1.60, Epsilon: 1.000\n",
            "Episode 100, P1 reward: 1.40, P2 reward: -1.40, Epsilon: 0.990\n",
            "Episode 200, P1 reward: -1.90, P2 reward: 3.00, Epsilon: 0.980\n",
            "Episode 300, P1 reward: 4.60, P2 reward: -0.60, Epsilon: 0.970\n",
            "Episode 400, P1 reward: 1.90, P2 reward: -1.90, Epsilon: 0.961\n",
            "Episode 500, P1 reward: 4.60, P2 reward: -2.30, Epsilon: 0.951\n",
            "Episode 600, P1 reward: 3.40, P2 reward: -1.10, Epsilon: 0.942\n",
            "Episode 700, P1 reward: 4.40, P2 reward: -1.90, Epsilon: 0.932\n",
            "Episode 800, P1 reward: 4.10, P2 reward: -1.60, Epsilon: 0.923\n",
            "Episode 900, P1 reward: -1.90, P2 reward: 3.50, Epsilon: 0.914\n",
            "Episode 1000, P1 reward: 4.00, P2 reward: -1.90, Epsilon: 0.905\n",
            "Episode 1100, P1 reward: -1.30, P2 reward: 4.50, Epsilon: 0.896\n",
            "Episode 1200, P1 reward: -4.20, P2 reward: 1.10, Epsilon: 0.887\n",
            "Episode 1300, P1 reward: 1.80, P2 reward: -5.50, Epsilon: 0.878\n",
            "Episode 1400, P1 reward: 3.50, P2 reward: -2.20, Epsilon: 0.869\n",
            "Episode 1500, P1 reward: 2.40, P2 reward: -4.50, Epsilon: 0.861\n",
            "Episode 1600, P1 reward: 3.50, P2 reward: -2.20, Epsilon: 0.852\n",
            "Episode 1700, P1 reward: -3.40, P2 reward: 3.50, Epsilon: 0.844\n",
            "Episode 1800, P1 reward: -3.60, P2 reward: 2.70, Epsilon: 0.835\n",
            "Episode 1900, P1 reward: 4.40, P2 reward: -1.30, Epsilon: 0.827\n",
            "Episode 2000, P1 reward: 3.70, P2 reward: -3.10, Epsilon: 0.819\n",
            "Episode 2100, P1 reward: 2.10, P2 reward: -2.80, Epsilon: 0.810\n",
            "Episode 2200, P1 reward: 0.90, P2 reward: -3.30, Epsilon: 0.802\n",
            "Episode 2300, P1 reward: -2.20, P2 reward: 4.80, Epsilon: 0.794\n",
            "Episode 2400, P1 reward: 4.20, P2 reward: -2.50, Epsilon: 0.787\n",
            "Episode 2500, P1 reward: 2.60, P2 reward: -4.80, Epsilon: 0.779\n",
            "Episode 2600, P1 reward: -2.90, P2 reward: 3.20, Epsilon: 0.771\n",
            "Episode 2700, P1 reward: -2.50, P2 reward: 2.20, Epsilon: 0.763\n",
            "Episode 2800, P1 reward: -3.90, P2 reward: 1.80, Epsilon: 0.756\n",
            "Episode 2900, P1 reward: -1.30, P2 reward: 4.20, Epsilon: 0.748\n",
            "Episode 3000, P1 reward: 4.40, P2 reward: -2.80, Epsilon: 0.741\n",
            "Episode 3100, P1 reward: -2.50, P2 reward: 3.40, Epsilon: 0.733\n",
            "Episode 3200, P1 reward: 3.60, P2 reward: -2.20, Epsilon: 0.726\n",
            "Episode 3300, P1 reward: 4.40, P2 reward: -1.30, Epsilon: 0.719\n",
            "Episode 3400, P1 reward: 3.20, P2 reward: -1.40, Epsilon: 0.712\n",
            "Episode 3500, P1 reward: 4.10, P2 reward: -1.60, Epsilon: 0.705\n",
            "Episode 3600, P1 reward: -0.20, P2 reward: 5.00, Epsilon: 0.698\n",
            "Episode 3700, P1 reward: -1.10, P2 reward: 4.20, Epsilon: 0.691\n",
            "Episode 3800, P1 reward: 4.90, P2 reward: -2.20, Epsilon: 0.684\n",
            "Episode 3900, P1 reward: 2.90, P2 reward: -0.90, Epsilon: 0.677\n",
            "Episode 4000, P1 reward: -3.10, P2 reward: 2.30, Epsilon: 0.670\n",
            "Episode 4100, P1 reward: -1.30, P2 reward: 4.40, Epsilon: 0.664\n",
            "Episode 4200, P1 reward: 3.90, P2 reward: -1.20, Epsilon: 0.657\n",
            "Episode 4300, P1 reward: 1.70, P2 reward: 4.80, Epsilon: 0.650\n",
            "Episode 4400, P1 reward: 0.50, P2 reward: -5.30, Epsilon: 0.644\n",
            "Episode 4500, P1 reward: 2.80, P2 reward: -4.30, Epsilon: 0.638\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m trained_model1, trained_model2 = \u001b[43mtrain_self_play\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mepsilon_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9999\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mtrain_self_play\u001b[39m\u001b[34m(episodes, epsilon, epsilon_min, epsilon_decay, gamma, batch_size, update_target_every)\u001b[39m\n\u001b[32m     67\u001b[39m     max_next_q = target_model(next_states).max(\u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m].unsqueeze(\u001b[32m1\u001b[39m)\n\u001b[32m     68\u001b[39m     target_q = rewards + gamma * max_next_q * (\u001b[32m1\u001b[39m - dones)\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m loss = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_q\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m optimizer.zero_grad()\n\u001b[32m     72\u001b[39m loss.backward()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ajmat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:3792\u001b[39m, in \u001b[36mmse_loss\u001b[39m\u001b[34m(input, target, size_average, reduce, reduction)\u001b[39m\n\u001b[32m   3789\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m   3791\u001b[39m expanded_input, expanded_target = torch.broadcast_tensors(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[32m-> \u001b[39m\u001b[32m3792\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexpanded_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3794\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "trained_model1, trained_model2 = train_self_play(episodes=100000,epsilon_decay=0.9999)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_model(model, path):\n",
        "    torch.save(model.state_dict(), path)\n",
        "    print(f\"Model saved to {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'trained_model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m save_model(\u001b[43mtrained_model\u001b[49m)\n",
            "\u001b[31mNameError\u001b[39m: name 'trained_model' is not defined"
          ]
        }
      ],
      "source": [
        "save_model(trained_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_trained_model(path=\"connect4_model_weights.pth\"):\n",
        "    model = DQN().to(device)\n",
        "    model.load_state_dict(torch.load(path, map_location=device))\n",
        "    model.eval()  # Set to evaluation mode\n",
        "    print(f\"Loaded model from {path}\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded model from connect4_dqn_ep10000.pth\n"
          ]
        }
      ],
      "source": [
        "trained_model = load_trained_model(path=\"connect4_dqn_ep10000.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32aOgNMqOSDp"
      },
      "source": [
        "Playing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "NGqA-mdHOTFx"
      },
      "outputs": [],
      "source": [
        "def play_against_model(starting_player=None):\n",
        "    model_choice = input(\"Play against model 1 (X) or model 2 (O)? Enter 1 or 2: \").strip()\n",
        "    model_path = \"connect4_model1_ep40000.pth\" if model_choice == \"1\" else \"connect4_model2_ep40000.pth\"\n",
        "    model = DQN().to(device)\n",
        "    env = Connect4Env()\n",
        "\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    if starting_player is None:\n",
        "        choice = input(\"Do you want to go first? (y/n): \").lower()\n",
        "        if choice == 'y':\n",
        "            env.current_player = -1\n",
        "        else:\n",
        "            env.current_player = 1\n",
        "    else:\n",
        "        env.current_player = -1 if starting_player == -1 else 1\n",
        "    print(\"You are Player -1 (O). Model is Player 1 (X). Columns: 0 to 6\")\n",
        "    print(f\"Playing against model from: {model_path}\")\n",
        "    print(env.board)\n",
        "\n",
        "    while not done:\n",
        "        if env.current_player == -1:\n",
        "            try:\n",
        "                user_action = int(input(\"Your move (0-6): \"))\n",
        "            except ValueError:\n",
        "                print(\"Invalid input. Try a number from 0 to 6.\")\n",
        "                continue\n",
        "            if user_action not in env.valid_actions():\n",
        "                print(\"Invalid move. Try again.\")\n",
        "                continue\n",
        "            state, reward, done, _ = env.step(user_action)\n",
        "        else:\n",
        "            valid_actions = env.valid_actions()\n",
        "            action = select_action(model, state, epsilon=0.0, valid_actions=valid_actions)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            print(\"Model played column:\", action)\n",
        "\n",
        "        # Always print board after any move\n",
        "        symbol_map = {1: 'X', -1: 'O', 0: '.'}\n",
        "        print(\"\\nCurrent board:\")\n",
        "        for row in env.board:\n",
        "            print(' '.join(symbol_map[cell] for cell in row))\n",
        "        print()\n",
        "\n",
        "        if done:\n",
        "            print(\"Game Over. Reward:\", reward)\n",
        "            break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scFE5r_6ObKi",
        "outputId": "f2b45771-82e3-4fd7-aec4-8dccf6e35df5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ajmat\\AppData\\Local\\Temp\\ipykernel_78084\\4261129178.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are Player -1 (O). Model is Player 1 (X). Columns: 0 to 6\n",
            "Playing against model from: connect4_model1_ep40000.pth\n",
            "[[0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n",
            "\n",
            "Current board:\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . O . . .\n",
            "\n",
            "Model played column: 6\n",
            "\n",
            "Current board:\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . O . . X\n",
            "\n",
            "\n",
            "Current board:\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . O . . .\n",
            ". . . O . . X\n",
            "\n",
            "Model played column: 6\n",
            "\n",
            "Current board:\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . O . . X\n",
            ". . . O . . X\n",
            "\n",
            "\n",
            "Current board:\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . O . . .\n",
            ". . . O . . X\n",
            ". . . O . . X\n",
            "\n",
            "Model played column: 6\n",
            "\n",
            "Current board:\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . O . . X\n",
            ". . . O . . X\n",
            ". . . O . . X\n",
            "\n",
            "\n",
            "Current board:\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . O . . .\n",
            ". . . O . . X\n",
            ". . . O . . X\n",
            ". . . O . . X\n",
            "\n",
            "Game Over. Reward: 1\n"
          ]
        }
      ],
      "source": [
        "play_against_model()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
