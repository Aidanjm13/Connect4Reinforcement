{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzH7tgStJksW"
      },
      "source": [
        "Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "M5HyqW2FJioP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "from copy import deepcopy\n",
        "import matplotlib.pyplot as plt\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(torch.cuda.is_available())\n",
        "print(\"Using device:\", device)\n",
        "if device.type == \"cuda\":\n",
        "    print(\"CUDA device name:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def to_device(tensor):\n",
        "    return tensor.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1c6KRsCJpvG"
      },
      "source": [
        "Class for connect4 environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "QfxUylEHJw5Q"
      },
      "outputs": [],
      "source": [
        "class Connect4Env:\n",
        "    def __init__(self):\n",
        "        self.rows = 6\n",
        "        self.cols = 7\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = np.zeros((self.rows, self.cols), dtype=int)\n",
        "        self.current_player = 1\n",
        "        self.last_move = None\n",
        "        return self._get_obs()\n",
        "\n",
        "    def _get_obs(self):\n",
        "        p1_board = (self.board == 1).astype(np.float32)\n",
        "        p2_board = (self.board == -1).astype(np.float32)\n",
        "        if self.current_player == 1:\n",
        "            return np.stack([p1_board, p2_board], axis=0)\n",
        "        else:\n",
        "            return np.stack([p2_board, p1_board], axis=0)\n",
        "\n",
        "    def valid_actions(self):\n",
        "        return [c for c in range(self.cols) if self.board[0, c] == 0]\n",
        "\n",
        "    def step(self, action):\n",
        "        if action not in self.valid_actions():\n",
        "            return self._get_obs(), -10, True, {}\n",
        "\n",
        "        for r in range(self.rows - 1, -1, -1):\n",
        "            if self.board[r, action] == 0:\n",
        "                self.board[r, action] = self.current_player\n",
        "                self.last_move = (r, action)\n",
        "                break\n",
        "\n",
        "        if self._check_winner(self.current_player, self.last_move):\n",
        "            return self._get_obs(), 1, True, {}\n",
        "\n",
        "        if len(self.valid_actions()) == 0:\n",
        "            return self._get_obs(), 0, True, {}\n",
        "\n",
        "        self.current_player *= -1\n",
        "        return self._get_obs(), -0.01, False, {}\n",
        "\n",
        "    def _check_winner(self, player, last_move):\n",
        "        if last_move is None:\n",
        "            return False\n",
        "        r, c = last_move\n",
        "        directions = [(1, 0), (0, 1), (1, 1), (1, -1)]\n",
        "        for dr, dc in directions:\n",
        "            count = 1\n",
        "            for d in [-1, 1]:\n",
        "                nr, nc = r + d * dr, c + d * dc\n",
        "                while 0 <= nr < self.rows and 0 <= nc < self.cols and self.board[nr, nc] == player:\n",
        "                    count += 1\n",
        "                    if count >= 4:\n",
        "                        return True\n",
        "                    nr += d * dr\n",
        "                    nc += d * dc\n",
        "        return False\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJdsV1swJ3lg"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "YaLFrL97J3QU"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(2, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 6 * 7, 128)\n",
        "        self.out = nn.Linear(128, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.out(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xljipnuZKGS1"
      },
      "source": [
        "Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Lf85V8OwKJta"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csLuvB18KKjR"
      },
      "source": [
        "Selecting Action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "AQMGWMxsKP1v"
      },
      "outputs": [],
      "source": [
        "# --- Training Functions ---\n",
        "def select_action(model, state, epsilon, valid_actions):\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice(valid_actions)\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            q_values = model(torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device))\n",
        "            q_values = q_values.squeeze()\n",
        "            q_values[[i for i in range(7) if i not in valid_actions]] = -float('inf')\n",
        "            return torch.argmax(q_values).item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_i6kGwpKSZ_"
      },
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "De4_ZhldKU00"
      },
      "outputs": [],
      "source": [
        "def train_self_play(episodes=50000, epsilon=1.0, epsilon_min=0.1, epsilon_decay=0.995,\n",
        "                     gamma=0.99, batch_size=64, update_target_every=10):\n",
        "    env = Connect4Env()\n",
        "    model1 = DQN().to(device)  # Player 1\n",
        "    model2 = DQN().to(device)  # Player -1\n",
        "    target_model1 = DQN().to(device)\n",
        "    target_model2 = DQN().to(device)\n",
        "    target_model1.load_state_dict(model1.state_dict())\n",
        "    target_model2.load_state_dict(model2.state_dict())\n",
        "    optimizer1 = optim.Adam(model1.parameters(), lr=1e-3)\n",
        "    optimizer2 = optim.Adam(model2.parameters(), lr=1e-3)\n",
        "    buffer1 = ReplayBuffer()\n",
        "    buffer2 = ReplayBuffer()\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        reward_p1 = 0\n",
        "        reward_p2 = 0\n",
        "\n",
        "        while not done:\n",
        "            current_player = env.current_player\n",
        "            model = model1 if current_player == 1 else model2\n",
        "            buffer = buffer1 if current_player == 1 else buffer2\n",
        "            optimizer = optimizer1 if current_player == 1 else optimizer2\n",
        "            target_model = target_model1 if current_player == 1 else target_model2\n",
        "\n",
        "            valid_actions = env.valid_actions()\n",
        "            action = select_action(model, state, epsilon, valid_actions)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            if done and reward == 1:\n",
        "                # Penalize the opponent for losing\n",
        "                opponent_buffer = buffer2 if current_player == 1 else buffer1\n",
        "                if len(opponent_buffer) > 0:\n",
        "                    last_state, last_action, _, last_next_state, last_done = opponent_buffer.buffer[-1]\n",
        "                    opponent_buffer.buffer[-1] = (last_state, last_action, -1.0, last_next_state, last_done)\n",
        "            buffer.push((np.array(state, copy=True), action, reward, np.array(next_state, copy=True), done))\n",
        "            state = next_state\n",
        "            if current_player == 1:\n",
        "                reward_p1 += reward\n",
        "            else:\n",
        "                reward_p2 += reward\n",
        "\n",
        "            if len(buffer) >= batch_size:\n",
        "                transitions = buffer.sample(batch_size)\n",
        "                states, actions, rewards, next_states, dones = zip(*transitions)\n",
        "\n",
        "                states = torch.tensor(np.stack(states), dtype=torch.float32).to(device)\n",
        "                actions = torch.tensor(actions).unsqueeze(1).to(device)\n",
        "                rewards = torch.tensor(rewards).unsqueeze(1).to(device)\n",
        "                next_states = torch.tensor(np.stack(next_states), dtype=torch.float32).to(device)\n",
        "                dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "\n",
        "                q_values = model(states).gather(1, actions)\n",
        "                with torch.no_grad():\n",
        "                    max_next_q = target_model(next_states).max(1)[0].unsqueeze(1)\n",
        "                    target_q = rewards + gamma * max_next_q * (1 - dones)\n",
        "\n",
        "                loss = F.mse_loss(q_values, target_q)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        if epsilon > epsilon_min:\n",
        "            epsilon *= epsilon_decay\n",
        "\n",
        "        if episode % update_target_every == 0:\n",
        "            target_model1.load_state_dict(model1.state_dict())\n",
        "            target_model2.load_state_dict(model2.state_dict())\n",
        "\n",
        "        if episode % 10000 == 0 and episode > 0:\n",
        "            torch.save(model1.state_dict(), f\"connect4_model1_ep{episode}.pth\")\n",
        "            torch.save(model2.state_dict(), f\"connect4_model2_ep{episode}.pth\")\n",
        "\n",
        "        if episode % 100 == 0: print(f\"Episode {episode}, P1 reward: {reward_p1:.2f}, P2 reward: {reward_p2:.2f}, Epsilon: {epsilon:.3f}\")\n",
        "\n",
        "    torch.save(model1.state_dict(), \"connect4_model1_final.pth\")\n",
        "    torch.save(model2.state_dict(), \"connect4_model2_final.pth\")\n",
        "    return model1, model2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXKXWYDSKYn1"
      },
      "source": [
        "Train It"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "8aeN-tkFKaRm",
        "outputId": "81958218-079e-4436-a746-783ec15a9370"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 0, Total reward: 0.82, Epsilon: 1.000\n",
            "Episode 1, Total reward: 0.79, Epsilon: 1.000\n",
            "Episode 2, Total reward: 0.78, Epsilon: 1.000\n",
            "Episode 3, Total reward: 0.69, Epsilon: 1.000\n",
            "Episode 4, Total reward: 0.82, Epsilon: 1.000\n",
            "Episode 5, Total reward: 0.73, Epsilon: 0.999\n",
            "Episode 6, Total reward: 0.82, Epsilon: 0.999\n",
            "Episode 7, Total reward: 0.66, Epsilon: 0.999\n",
            "Episode 8, Total reward: 0.69, Epsilon: 0.999\n",
            "Episode 9, Total reward: 0.84, Epsilon: 0.999\n",
            "Episode 10, Total reward: 0.88, Epsilon: 0.999\n",
            "Episode 11, Total reward: 0.74, Epsilon: 0.999\n",
            "Episode 12, Total reward: 0.85, Epsilon: 0.999\n",
            "Episode 13, Total reward: 0.70, Epsilon: 0.999\n",
            "Episode 14, Total reward: 0.76, Epsilon: 0.999\n",
            "Episode 15, Total reward: 0.83, Epsilon: 0.998\n",
            "Episode 16, Total reward: 0.84, Epsilon: 0.998\n",
            "Episode 17, Total reward: 0.71, Epsilon: 0.998\n",
            "Episode 18, Total reward: 0.90, Epsilon: 0.998\n",
            "Episode 19, Total reward: 0.70, Epsilon: 0.998\n",
            "Episode 20, Total reward: 0.87, Epsilon: 0.998\n",
            "Episode 21, Total reward: 0.89, Epsilon: 0.998\n",
            "Episode 22, Total reward: 0.81, Epsilon: 0.998\n",
            "Episode 23, Total reward: 0.78, Epsilon: 0.998\n",
            "Episode 24, Total reward: 0.79, Epsilon: 0.998\n",
            "Episode 25, Total reward: 0.90, Epsilon: 0.997\n",
            "Episode 26, Total reward: 0.92, Epsilon: 0.997\n",
            "Episode 27, Total reward: 0.76, Epsilon: 0.997\n",
            "Episode 28, Total reward: 0.74, Epsilon: 0.997\n",
            "Episode 29, Total reward: 0.73, Epsilon: 0.997\n",
            "Episode 30, Total reward: 0.68, Epsilon: 0.997\n",
            "Episode 31, Total reward: 0.77, Epsilon: 0.997\n",
            "Episode 32, Total reward: 0.81, Epsilon: 0.997\n",
            "Episode 33, Total reward: 0.77, Epsilon: 0.997\n",
            "Episode 34, Total reward: 0.89, Epsilon: 0.997\n",
            "Episode 35, Total reward: 0.69, Epsilon: 0.996\n",
            "Episode 36, Total reward: 0.71, Epsilon: 0.996\n",
            "Episode 37, Total reward: 0.77, Epsilon: 0.996\n",
            "Episode 38, Total reward: 0.86, Epsilon: 0.996\n",
            "Episode 39, Total reward: 0.87, Epsilon: 0.996\n",
            "Episode 40, Total reward: 0.84, Epsilon: 0.996\n",
            "Episode 41, Total reward: 0.79, Epsilon: 0.996\n",
            "Episode 42, Total reward: 0.70, Epsilon: 0.996\n",
            "Episode 43, Total reward: 0.85, Epsilon: 0.996\n",
            "Episode 44, Total reward: 0.85, Epsilon: 0.996\n",
            "Episode 45, Total reward: 0.80, Epsilon: 0.995\n",
            "Episode 46, Total reward: 0.77, Epsilon: 0.995\n",
            "Episode 47, Total reward: 0.73, Epsilon: 0.995\n",
            "Episode 48, Total reward: 0.84, Epsilon: 0.995\n",
            "Episode 49, Total reward: 0.94, Epsilon: 0.995\n",
            "Episode 50, Total reward: 0.73, Epsilon: 0.995\n",
            "Episode 51, Total reward: 0.64, Epsilon: 0.995\n",
            "Episode 52, Total reward: 0.86, Epsilon: 0.995\n",
            "Episode 53, Total reward: 0.72, Epsilon: 0.995\n",
            "Episode 54, Total reward: 0.76, Epsilon: 0.995\n",
            "Episode 55, Total reward: 0.76, Epsilon: 0.994\n",
            "Episode 56, Total reward: 0.72, Epsilon: 0.994\n",
            "Episode 57, Total reward: 0.82, Epsilon: 0.994\n",
            "Episode 58, Total reward: 0.84, Epsilon: 0.994\n",
            "Episode 59, Total reward: 0.71, Epsilon: 0.994\n",
            "Episode 60, Total reward: 0.89, Epsilon: 0.994\n",
            "Episode 61, Total reward: 0.88, Epsilon: 0.994\n",
            "Episode 62, Total reward: 0.75, Epsilon: 0.994\n",
            "Episode 63, Total reward: 0.92, Epsilon: 0.994\n",
            "Episode 64, Total reward: 0.88, Epsilon: 0.994\n",
            "Episode 65, Total reward: 0.78, Epsilon: 0.993\n",
            "Episode 66, Total reward: 0.85, Epsilon: 0.993\n",
            "Episode 67, Total reward: 0.94, Epsilon: 0.993\n",
            "Episode 68, Total reward: 0.88, Epsilon: 0.993\n",
            "Episode 69, Total reward: 0.79, Epsilon: 0.993\n",
            "Episode 70, Total reward: 0.83, Epsilon: 0.993\n",
            "Episode 71, Total reward: 0.88, Epsilon: 0.993\n",
            "Episode 72, Total reward: 0.85, Epsilon: 0.993\n",
            "Episode 73, Total reward: 0.84, Epsilon: 0.993\n",
            "Episode 74, Total reward: 0.88, Epsilon: 0.993\n",
            "Episode 75, Total reward: 0.85, Epsilon: 0.992\n",
            "Episode 76, Total reward: 0.91, Epsilon: 0.992\n",
            "Episode 77, Total reward: 0.60, Epsilon: 0.992\n",
            "Episode 78, Total reward: 0.77, Epsilon: 0.992\n",
            "Episode 79, Total reward: 0.74, Epsilon: 0.992\n",
            "Episode 80, Total reward: 0.80, Epsilon: 0.992\n",
            "Episode 81, Total reward: 0.74, Epsilon: 0.992\n",
            "Episode 82, Total reward: 0.83, Epsilon: 0.992\n",
            "Episode 83, Total reward: 0.94, Epsilon: 0.992\n",
            "Episode 84, Total reward: 0.70, Epsilon: 0.992\n",
            "Episode 85, Total reward: 0.75, Epsilon: 0.991\n",
            "Episode 86, Total reward: 0.79, Epsilon: 0.991\n",
            "Episode 87, Total reward: 0.87, Epsilon: 0.991\n",
            "Episode 88, Total reward: 0.76, Epsilon: 0.991\n",
            "Episode 89, Total reward: 0.85, Epsilon: 0.991\n",
            "Episode 90, Total reward: 0.67, Epsilon: 0.991\n",
            "Episode 91, Total reward: 0.94, Epsilon: 0.991\n",
            "Episode 92, Total reward: 0.65, Epsilon: 0.991\n",
            "Episode 93, Total reward: 0.87, Epsilon: 0.991\n",
            "Episode 94, Total reward: 0.83, Epsilon: 0.991\n",
            "Episode 95, Total reward: 0.72, Epsilon: 0.990\n",
            "Episode 96, Total reward: 0.72, Epsilon: 0.990\n",
            "Episode 97, Total reward: 0.85, Epsilon: 0.990\n",
            "Episode 98, Total reward: 0.90, Epsilon: 0.990\n",
            "Episode 99, Total reward: 0.85, Epsilon: 0.990\n",
            "Episode 100, Total reward: 0.65, Epsilon: 0.990\n",
            "Episode 101, Total reward: 0.70, Epsilon: 0.990\n",
            "Episode 102, Total reward: 0.77, Epsilon: 0.990\n",
            "Episode 103, Total reward: 0.87, Epsilon: 0.990\n",
            "Episode 104, Total reward: 0.90, Epsilon: 0.990\n",
            "Episode 105, Total reward: 0.68, Epsilon: 0.989\n",
            "Episode 106, Total reward: 0.85, Epsilon: 0.989\n",
            "Episode 107, Total reward: 0.88, Epsilon: 0.989\n",
            "Episode 108, Total reward: 0.77, Epsilon: 0.989\n",
            "Episode 109, Total reward: 0.74, Epsilon: 0.989\n",
            "Episode 110, Total reward: 0.86, Epsilon: 0.989\n",
            "Episode 111, Total reward: 0.85, Epsilon: 0.989\n",
            "Episode 112, Total reward: 0.84, Epsilon: 0.989\n",
            "Episode 113, Total reward: 0.88, Epsilon: 0.989\n",
            "Episode 114, Total reward: 0.72, Epsilon: 0.989\n",
            "Episode 115, Total reward: 0.87, Epsilon: 0.988\n",
            "Episode 116, Total reward: 0.83, Epsilon: 0.988\n",
            "Episode 117, Total reward: 0.84, Epsilon: 0.988\n",
            "Episode 118, Total reward: 0.88, Epsilon: 0.988\n",
            "Episode 119, Total reward: 0.78, Epsilon: 0.988\n",
            "Episode 120, Total reward: 0.81, Epsilon: 0.988\n",
            "Episode 121, Total reward: 0.64, Epsilon: 0.988\n",
            "Episode 122, Total reward: 0.76, Epsilon: 0.988\n",
            "Episode 123, Total reward: 0.68, Epsilon: 0.988\n",
            "Episode 124, Total reward: 0.93, Epsilon: 0.988\n",
            "Episode 125, Total reward: 0.80, Epsilon: 0.987\n",
            "Episode 126, Total reward: 0.86, Epsilon: 0.987\n",
            "Episode 127, Total reward: 0.73, Epsilon: 0.987\n",
            "Episode 128, Total reward: 0.91, Epsilon: 0.987\n",
            "Episode 129, Total reward: 0.83, Epsilon: 0.987\n",
            "Episode 130, Total reward: 0.88, Epsilon: 0.987\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m trained_model = \u001b[43mtrain_self_play\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mepsilon_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9999\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m save_model(trained_model)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mtrain_self_play\u001b[39m\u001b[34m(episodes, epsilon, epsilon_min, epsilon_decay, gamma, batch_size, update_target_every)\u001b[39m\n\u001b[32m     49\u001b[39m         loss = F.mse_loss(q_values, target_q)\n\u001b[32m     50\u001b[39m         optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m         optimizer.step()\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m epsilon > epsilon_min:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ajmat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ajmat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ajmat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "trained_model = train_self_play(episodes=100000,epsilon_decay=0.9999)\n",
        "save_model(trained_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_model(model, path=\"connect4_model_weights.pth\"):\n",
        "    torch.save(model.state_dict(), path)\n",
        "    print(f\"Model saved to {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to connect4_model_weights.pth\n"
          ]
        }
      ],
      "source": [
        "save_model(trained_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_trained_model(path=\"connect4_model_weights.pth\"):\n",
        "    model = DQN().to(device)\n",
        "    model.load_state_dict(torch.load(path, map_location=device))\n",
        "    model.eval()  # Set to evaluation mode\n",
        "    print(f\"Loaded model from {path}\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded model from connect4_dqn_ep10000.pth\n"
          ]
        }
      ],
      "source": [
        "trained_model = load_trained_model(path=\"connect4_dqn_ep10000.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32aOgNMqOSDp"
      },
      "source": [
        "Playing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGqA-mdHOTFx"
      },
      "outputs": [],
      "source": [
        "def play_against_model(starting_player=None):\n",
        "    model_choice = input(\"Play against model 1 (X) or model 2 (O)? Enter 1 or 2: \").strip()\n",
        "    model_path = \"connect4_model1_final.pth\" if model_choice == \"1\" else \"connect4_model2_final.pth\"\n",
        "\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    if starting_player is None:\n",
        "        choice = input(\"Do you want to go first? (y/n): \").lower()\n",
        "        if choice == 'y':\n",
        "            env.current_player = -1\n",
        "        else:\n",
        "            env.current_player = 1\n",
        "    else:\n",
        "        env.current_player = -1 if starting_player == -1 else 1\n",
        "    print(\"You are Player -1 (O). Model is Player 1 (X). Columns: 0 to 6\")\n",
        "    print(f\"Playing against model from: {model_path}\")\n",
        "    print(env.board)\n",
        "\n",
        "    while not done:\n",
        "        if env.current_player == -1:\n",
        "            try:\n",
        "                user_action = int(input(\"Your move (0-6): \"))\n",
        "            except ValueError:\n",
        "                print(\"Invalid input. Try a number from 0 to 6.\")\n",
        "                continue\n",
        "            if user_action not in env.valid_actions():\n",
        "                print(\"Invalid move. Try again.\")\n",
        "                continue\n",
        "            state, reward, done, _ = env.step(user_action)\n",
        "        else:\n",
        "            valid_actions = env.valid_actions()\n",
        "            action = select_action(model, state, epsilon=0.0, valid_actions=valid_actions)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            print(\"Model played column:\", action)\n",
        "\n",
        "        symbol_map = {1: 'X', -1: 'O', 0: '.'}\n",
        "        for row in env.board:\n",
        "            print(' '.join(symbol_map[cell] for cell in row))\n",
        "        if done:\n",
        "            print(\"Game Over. Reward:\", reward)\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scFE5r_6ObKi",
        "outputId": "f2b45771-82e3-4fd7-aec4-8dccf6e35df5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are Player -1 (O). Model is Player 1 (X). Columns: 0 to 6\n",
            "[[0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . O . . .\n",
            "Model played column: 3\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . X . . .\n",
            ". . . O . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . O . . .\n",
            ". . . X . . .\n",
            ". . . O . . .\n",
            "Model played column: 6\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . O . . .\n",
            ". . . X . . .\n",
            ". . . O . . X\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . O . . .\n",
            ". . . X . . .\n",
            ". . . O O . X\n",
            "Model played column: 6\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . O . . .\n",
            ". . . X . . X\n",
            ". . . O O . X\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . O . . .\n",
            ". . . X . . X\n",
            ". . O O O . X\n",
            "Model played column: 3\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . X . . .\n",
            ". . . O . . .\n",
            ". . . X . . X\n",
            ". . O O O . X\n",
            ". . . . . . .\n",
            ". . . . . . .\n",
            ". . . X . . .\n",
            ". . . O . . .\n",
            ". . . X . . X\n",
            ". O O O O . X\n",
            "Game Over. Reward: 1\n"
          ]
        }
      ],
      "source": [
        "play_against_model(modelPath=\"connect4_dqn_ep10000.pth\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
